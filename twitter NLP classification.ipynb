{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, auc, roc_auc_score\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import scipy\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1578614, 2)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('C:/Users/vbigand/Downloads/twitter_sentiment.csv', encoding='latin1', usecols=['Sentiment','SentimentText'])\n",
    "df.columns = ['sentiment','text']\n",
    "df = df.sample(frac=1,random_state=1)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 @OtaliaRocks You mean looky \n",
      "0 @lecruetlecuit I saw that \n",
      "1 @SoulIsTheGoal not yet doh! watching Big Bang Theory S1 dan lent me \\o/ \n",
      "0 @xbllygbsn lol saaame :$. wish they were healthy  lol :$. aah okayy :$. dont drink bottled water from asda then ;)\n",
      "1 I'm pleased with myself. I've only just got up and it's 10:03. Score! \n",
      "1 @elissaalva HEEEEYYYYY GIIIRRRRRRLLLLL!!!!!!! I hope all is well with you \n",
      "0 Napped and missed the gym! Busy day, bed early! Class tomorrow \n",
      "0 @feybee Haha g2g to sleep now  my cus said so ttyl love ya!! Good Noght God Bless!!\n",
      "1 I am contimplating throwing myself a birthday part @ a rollerskating rink to force all my friends to rollerskate with me  who would come?\n",
      "1 #jonasnewsongs favorites: fly with me and much better. you boys rock  marry me nick?\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(df.iloc[i]['sentiment'],df.iloc[i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 @OtaliaRocks You mean looky \n",
      "0 @lecruetlecuit I saw that \n",
      "1 @SoulIsTheGoal not yet doh! watching Big Bang Theory S1 dan lent me \\o/ \n",
      "0 @xbllygbsn lol saaame :$. wish they were healthy  lol :$. aah okayy :$. dont drink bottled water from asda then ;)\n",
      "1 I'm pleased with myself. I've only just got up and it's 10:03. Score! \n",
      "1 @elissaalva HEEEEYYYYY GIIIRRRRRRLLLLL!!!!!!! I hope all is well with you \n",
      "0 Napped and missed the gym! Busy day, bed early! Class tomorrow \n",
      "0 @feybee Haha g2g to sleep now  my cus said so ttyl love ya!! Good Noght God Bless!!\n",
      "1 I am contimplating throwing myself a birthday part @ a rollerskating rink to force all my friends to rollerskate with me  who would come?\n",
      "1 #jonasnewsongs favorites: fly with me and much better. you boys rock  marry me nick?\n"
     ]
    }
   ],
   "source": [
    "#other method\n",
    "for row in df.head(10).iterrows():\n",
    "    print(row[1].sentiment,row[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re.sub let us replace the pattern by the string of your choice\n",
    "def tokenize(tweet):\n",
    "    tweet = re.sub(r'http\\S+','',tweet)\n",
    "    tweet = re.sub(r'@(\\w+)','',tweet)\n",
    "    tweet = re.sub(r'#(\\w+)','',tweet)\n",
    "    tweet = re.sub(r'[^\\w\\s]', '', tweet)\n",
    "    tweet = tweet.strip().lower()\n",
    "    tokens = word_tokenize(tweet)\n",
    "    return tokens   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df.text.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text'] = df['tokens'].map(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1578614, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>you mean looky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i saw that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>not yet doh watching big bang theory s1 dan le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>lol saaame wish they were healthy lol aah okay...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>im pleased with myself ive only just got up an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                       cleaned_text\n",
       "0          1                                     you mean looky\n",
       "1          0                                         i saw that\n",
       "2          1  not yet doh watching big bang theory s1 dan le...\n",
       "3          0  lol saaame wish they were healthy lol aah okay...\n",
       "4          1  im pleased with myself ive only just got up an..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['sentiment','cleaned_text']].to_csv('C:/Users/vbigand/Downloads/clean_twitter_text')\n",
    "\n",
    "df = pd.read_csv('C:/Users/vbigand/Downloads/clean_twitter_text')\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (1420752,)\ty_train: (1420752,)\n",
      "X_test: (157862,)\ty_test: (157862,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'].values.astype('str'),df['sentiment'].values.astype('str'),random_state=42, test_size = 0.1, stratify = df['sentiment'])\n",
    "print (\"X_train: {}\\ty_train: {}\\nX_test: {}\\ty_test: {}\".format(X_train.shape,y_train.shape,X_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(max_features=40000,min_df=5,max_df=0.8,stop_words='english',analyzer='word',ngram_range=(1,2))\n",
    "vect.fit(X_train)\n",
    "tfidf_mat_x_train = vect.transform(X_train)\n",
    "tfidf_mat_x_test = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LogisticRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-65f2c0655d98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sag'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmulticlass\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_mat_x_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(solver='sag',verbose=2)\n",
    "clf.fit(tfidf_mat_x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.781480026858902\n"
     ]
    }
   ],
   "source": [
    "y_predict = clf.predict(tfidf_mat_x_test)\n",
    "print('Accuracy score: {}'.format(accuracy_score(y_pred=y_predict,y_true=y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convergence after 21 epochs took 78 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.8047471842495344\n"
     ]
    }
   ],
   "source": [
    "vect = TfidfVectorizer(max_features=40000,min_df=5,max_df=0.5,stop_words='english',analyzer='char',ngram_range=(1,4))\n",
    "vect.fit(X_train)\n",
    "tfidf_mat_x_train_char = vect.transform(X_train)\n",
    "tfidf_mat_x_test_char = vect.transform(X_test)\n",
    "clf = LogisticRegression(solver='sag',verbose=2)\n",
    "clf.fit(tfidf_mat_x_train,y_train)\n",
    "y_predict = clf.predict(tfidf_mat_x_test)\n",
    "print('Accuracy score: {}'.format(accuracy_score(y_pred=y_predict,y_true=y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word + Char tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-3f774879d3d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtfidf_word_char_x_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_mat_x_train_char\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidf_mat_x_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtfidf_word_char_x_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_mat_x_test_char\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidf_mat_x_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sag'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_word_char_x_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_predict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf_word_char_x_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\construct.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m     \"\"\"\n\u001b[1;32m--> 464\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbmat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\construct.py\u001b[0m in \u001b[0;36mbmat\u001b[1;34m(blocks, format, dtype)\u001b[0m\n\u001b[0;32m    605\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow_offsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_offsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m     \u001b[0midx_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_index_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m     \u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnnz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tfidf_word_char_x_train = hstack((tfidf_mat_x_train_char,tfidf_mat_x_train))\n",
    "tfidf_word_char_x_test = hstack((tfidf_mat_x_test_char,tfidf_mat_x_test))\n",
    "clf = LogisticRegression(solver='sag',verbose=2)\n",
    "clf.fit(tfidf_word_char_x_train, y_train)\n",
    "y_predict = clf.predict(tfidf_word_char_x_test)\n",
    "print('Accuracy score: {}'.format(accuracy_score(y_pred=y_predict,y_true=y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
